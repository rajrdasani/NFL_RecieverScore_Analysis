---
title: "CatchScorePredictionModels"
output: html_document
date: "2023-08-15"
---



```{r}
library(dplyr)
library(tidyr)
library(ggplot2)
library(reshape2) 
library(ggrepel)
```


```{r}
data = read.csv("../Data/WR_Advanced_Stats - PivotData_Clean.csv")
head(data)
```

Create table that has player column for a single year, with the deltas of the previous year

Yards columns loaded in as character, so change them 
```{r}
data = data %>%
  mutate_at(c('X2017_Yards', 'X2018_Yards', 'X2019_Yards', 'X2020_Yards', 'X2021_Yards', 'X2022_Yards'), readr::parse_number)
```

```{r}
wrscore_21_22 = data %>%
  select(Player, 
         X2021_Team, X2021_Yards, X2021_YardsRun, X2021_Targets, X2021_YardsPerRoute, X2021_OpenScore, X2021_CatchScore, X2021_YACScore, X2021_Overall,
         X2022_Team, X2022_Yards, X2022_YardsRun, X2022_Targets, X2022_YardsPerRoute, X2022_OpenScore, X2022_CatchScore, X2022_YACScore, X2022_Overall
         ) %>%
  mutate(
    team_change = 1 * (X2021_Team == X2022_Team), 
    yards_delta = X2022_Yards - X2021_Yards, 
    yardsrun_delta = X2022_YardsRun - X2021_YardsRun,
    targets_delta = X2022_Targets - X2021_Targets,
    yardsperroute_delta = X2022_YardsPerRoute - X2021_YardsPerRoute,
    openscore_delta = X2022_OpenScore - X2021_OpenScore,
    catchscore_delta = X2022_CatchScore - X2021_CatchScore,
    yacscore_delta = X2022_YACScore - X2021_YACScore,
    overall_delta = X2022_Overall - X2021_Overall) %>%
  na.exclude() %>%
  mutate(Player = paste(Player, "21-22")) %>%
  rename(Y0_Yards = X2021_Yards,
         Y1_Yards = X2022_Yards,
         Y0_YardsRun = X2021_YardsRun,
         Y1_YardsRun = X2022_YardsRun,
         Y0_Targets = X2021_Targets,
         Y1_Targets = X2022_Targets,
         Y0_YardsPerRoute = X2021_YardsPerRoute,
         Y1_YardsPerRoute = X2022_YardsPerRoute,
         Y0_CatchScore = X2021_CatchScore,
         Y1_CatchScore = X2022_CatchScore,
         Y0_OpenScore = X2021_OpenScore,
         Y1_OpenScore = X2022_OpenScore,
         Y0_YACScore = X2021_YACScore,
         Y1_YACScore = X2022_YACScore,
         Y0_Overall = X2021_Overall,
         Y1_Overall = X2022_Overall) %>%
  select(-X2021_Team, -X2022_Team)
   

wrscore_20_21 = data %>%
  select(Player, 
         X2021_Team, X2021_Yards, X2021_YardsRun, X2021_Targets, X2021_YardsPerRoute, X2021_OpenScore, X2021_CatchScore, X2021_YACScore, X2021_Overall,
         X2020_Team, X2020_Yards, X2020_YardsRun, X2020_Targets, X2020_YardsPerRoute, X2020_OpenScore, X2020_CatchScore, X2020_YACScore, X2020_Overall
         ) %>%
  mutate(
    team_change = 1 * (X2021_Team == X2020_Team), 
    yards_delta = X2021_Yards - X2020_Yards, 
    yardsrun_delta = X2021_YardsRun - X2020_YardsRun,
    targets_delta = X2021_Targets - X2020_Targets,
    yardsperroute_delta = X2021_YardsPerRoute - X2020_YardsPerRoute,
    openscore_delta = X2021_OpenScore - X2020_OpenScore,
    catchscore_delta = X2021_CatchScore - X2020_CatchScore,
    yacscore_delta = X2021_YACScore - X2020_YACScore,
    overall_delta = X2021_Overall - X2020_Overall) %>%
  na.exclude() %>%
  mutate(Player = paste(Player, "20-21")) %>%
  rename(Y0_Yards = X2020_Yards,
         Y1_Yards = X2021_Yards,
         Y0_YardsRun = X2020_YardsRun,
         Y1_YardsRun = X2021_YardsRun,
         Y0_Targets = X2020_Targets,
         Y1_Targets = X2021_Targets,
         Y0_YardsPerRoute = X2020_YardsPerRoute,
         Y1_YardsPerRoute = X2021_YardsPerRoute,
         Y0_CatchScore = X2020_CatchScore,
         Y1_CatchScore = X2021_CatchScore,
         Y0_OpenScore = X2020_OpenScore,
         Y1_OpenScore = X2021_OpenScore,
         Y0_YACScore = X2020_YACScore,
         Y1_YACScore = X2021_YACScore,
         Y0_Overall = X2020_Overall,
         Y1_Overall = X2021_Overall) %>%
  select(-X2021_Team, -X2020_Team)

wrscore_19_20 = data %>%
  select(Player, 
         X2019_Team, X2019_Yards, X2019_YardsRun, X2019_Targets, X2019_YardsPerRoute, X2019_OpenScore, X2019_CatchScore, X2019_YACScore, X2019_Overall,
         X2020_Team, X2020_Yards, X2020_YardsRun, X2020_Targets, X2020_YardsPerRoute, X2020_OpenScore, X2020_CatchScore, X2020_YACScore, X2020_Overall
         ) %>%
  mutate(
    team_change = 1 * (X2019_Team == X2020_Team), 
    yards_delta = X2020_Yards - X2019_Yards, 
    yardsrun_delta = X2020_YardsRun - X2019_YardsRun,
    targets_delta = X2020_Targets - X2019_Targets,
    yardsperroute_delta = X2020_YardsPerRoute - X2019_YardsPerRoute,
    openscore_delta = X2020_OpenScore - X2019_OpenScore,
    catchscore_delta = X2020_CatchScore - X2019_CatchScore,
    yacscore_delta = X2020_YACScore - X2019_YACScore,
    overall_delta = X2020_Overall - X2019_Overall) %>%
  na.exclude() %>%
  mutate(Player = paste(Player, "19-20")) %>%
  rename(Y0_Yards = X2019_Yards,
         Y1_Yards = X2020_Yards,
         Y0_YardsRun = X2019_YardsRun,
         Y1_YardsRun = X2020_YardsRun,
         Y0_Targets = X2019_Targets,
         Y1_Targets = X2020_Targets,
         Y0_YardsPerRoute = X2019_YardsPerRoute,
         Y1_YardsPerRoute = X2020_YardsPerRoute,
         Y0_CatchScore = X2019_CatchScore,
         Y1_CatchScore = X2020_CatchScore,
         Y0_OpenScore = X2019_OpenScore,
         Y1_OpenScore = X2020_OpenScore,
         Y0_YACScore = X2019_YACScore,
         Y1_YACScore = X2020_YACScore,
         Y0_Overall = X2019_Overall,
         Y1_Overall = X2020_Overall) %>%
  select(-X2019_Team, -X2020_Team)

wrscore_18_19 = data %>%
  select(Player, 
         X2019_Team, X2019_Yards, X2019_YardsRun, X2019_Targets, X2019_YardsPerRoute, X2019_OpenScore, X2019_CatchScore, X2019_YACScore, X2019_Overall,
         X2018_Team, X2018_Yards, X2018_YardsRun, X2018_Targets, X2018_YardsPerRoute, X2018_OpenScore, X2018_CatchScore, X2018_YACScore, X2018_Overall
         ) %>%
  mutate(
    team_change = 1 * (X2019_Team == X2018_Team), 
    yards_delta = X2019_Yards - X2018_Yards, 
    yardsrun_delta = X2019_YardsRun - X2018_YardsRun,
    targets_delta = X2019_Targets - X2018_Targets,
    yardsperroute_delta = X2019_YardsPerRoute - X2018_YardsPerRoute,
    openscore_delta = X2019_OpenScore - X2018_OpenScore,
    catchscore_delta = X2019_CatchScore - X2018_CatchScore,
    yacscore_delta = X2019_YACScore - X2018_YACScore,
    overall_delta = X2019_Overall - X2018_Overall) %>%
  na.exclude() %>%
  mutate(Player = paste(Player, "18-19")) %>%
  rename(Y0_Yards = X2018_Yards,
         Y1_Yards = X2019_Yards,
         Y0_YardsRun = X2018_YardsRun,
         Y1_YardsRun = X2019_YardsRun,
         Y0_Targets = X2018_Targets,
         Y1_Targets = X2019_Targets,
         Y0_YardsPerRoute = X2018_YardsPerRoute,
         Y1_YardsPerRoute = X2019_YardsPerRoute,
         Y0_CatchScore = X2018_CatchScore,
         Y1_CatchScore = X2019_CatchScore,
         Y0_OpenScore = X2018_OpenScore,
         Y1_OpenScore = X2019_OpenScore,
         Y0_YACScore = X2018_YACScore,
         Y1_YACScore = X2019_YACScore,
         Y0_Overall = X2018_Overall,
         Y1_Overall = X2019_Overall) %>%
  select(-X2019_Team, -X2018_Team)

wrscore_17_18 = data %>%
  select(Player, 
         X2017_Team, X2017_Yards, X2017_YardsRun, X2017_Targets, X2017_YardsPerRoute, X2017_OpenScore, X2017_CatchScore, X2017_YACScore, X2017_Overall,
         X2018_Team, X2018_Yards, X2018_YardsRun, X2018_Targets, X2018_YardsPerRoute, X2018_OpenScore, X2018_CatchScore, X2018_YACScore, X2018_Overall
         ) %>%
  mutate(
    team_change = 1 * (X2017_Team == X2018_Team), 
    yards_delta = X2018_Yards - X2017_Yards, 
    yardsrun_delta = X2018_YardsRun - X2017_YardsRun,
    targets_delta = X2018_Targets - X2017_Targets,
    yardsperroute_delta = X2018_YardsPerRoute - X2017_YardsPerRoute,
    openscore_delta = X2018_OpenScore - X2017_OpenScore,
    catchscore_delta = X2018_CatchScore - X2017_CatchScore,
    yacscore_delta = X2018_YACScore - X2017_YACScore,
    overall_delta = X2018_Overall - X2017_Overall) %>%
  na.exclude() %>%
  mutate(Player = paste(Player, "17-18")) %>%
  rename(Y0_Yards = X2017_Yards,
         Y1_Yards = X2018_Yards,
         Y0_YardsRun = X2017_YardsRun,
         Y1_YardsRun = X2018_YardsRun,
         Y0_Targets = X2017_Targets,
         Y1_Targets = X2018_Targets,
         Y0_YardsPerRoute = X2017_YardsPerRoute,
         Y1_YardsPerRoute = X2018_YardsPerRoute,
         Y0_CatchScore = X2017_CatchScore,
         Y1_CatchScore = X2018_CatchScore,
         Y0_OpenScore = X2017_OpenScore,
         Y1_OpenScore = X2018_OpenScore,
         Y0_YACScore = X2017_YACScore,
         Y1_YACScore = X2018_YACScore,
         Y0_Overall = X2017_Overall,
         Y1_Overall = X2018_Overall) %>%
  select(-X2017_Team, -X2018_Team)



fulldata_deltas = rbind(wrscore_21_22, wrscore_20_21, wrscore_19_20, wrscore_18_19, wrscore_17_18)
```

Split into train/test
```{r}
set.seed(502)

sample <- sample(c(TRUE, FALSE), nrow(x_var), replace=TRUE, prob=c(0.8,0.2))
train <- fulldata_deltas[sample, ]
test <- fulldata_deltas[!sample, ]
```




Quick feature selection

Team Change Variable

```{r}
gg <- ggplot(fulldata_deltas, aes(Y0_Overall, Y1_Overall, colour = as.factor(team_change))) + 
  geom_point() + 
  geom_smooth(alpha=0.3, method="lm") + 
  labs(title = "Year 0 Overall WR Score vs Year 1 Overall WR Score",
       subtitle = "Players who did not change teams had a more consistent 
       catch score on average compared to players who did change.")
print(gg)
```







Subset y-variable as Year 1 (vs Year 0) OverallScore
Subset x-variables as all Y0 data and team_change

Linear Regression Model (only previous year numbers)
```{r}
wrscore.lm <- lm(formula = Y1_Overall ~ Y0_Yards + Y0_YardsRun + Y0_Targets + Y0_YardsPerRoute + Y0_CatchScore + Y0_OpenScore + Y0_YACScore + Y0_Overall + team_change,
                 data = train)

print(summary(wrscore.lm))

#plot residuals
plot(wrscore.lm$residuals, pch = 16, col = "red")
```
Overall it does ok with an R**2 of 0.3234. However, outside of the previous year's overall score, nothing else had a strong prediction for overall.

```{r}
#predictions
test$linreg_Y1_Overall_prediction = predict(wrscore.lm, test)
test$linreg_residual = abs(test$Y1_Overall - test$linreg_Y1_Overall_prediction)
test[,c('Player','Y0_Overall', 'Y1_Overall','linreg_Y1_Overall_prediction','linreg_residual')]

#Plot Test Data
ggplot(test, aes(Y1_Overall, linreg_Y1_Overall_prediction, label = Player)) +
  geom_point() + 
  geom_text_repel(aes(label=ifelse(linreg_residual>15,as.character(Player),''))) +
  geom_abline(slope=1, intercept = 0, color = 'black') +
  geom_text(aes(50, 90, label = "Actual Score Worse than Predicted Score",), color = 'red') +
  geom_text(aes(80, 30, label = "Actual Score Better than Predicted Score"), color = 'lightgreen') +
  labs(title = "Player Year 1 (2nd of 2 Consec. Years) WR Score vs Linear Regression Year 1 WR Score Prediction",
         subtitle = "Only Test Data graphed, Players labeled have incorrect predictions of 15+",
         x = "Actual Year 1 WR Score", 
         y = "Predicted Year 1 WR Score") +
  theme_minimal() +
    xlim(20,100) +
    ylim(20, 100)

ggsave("../Images/LinearRegression_Year1Predictions.png", bg="white")
```
Lin Reg RMSE
```{r}
sqrt(mean(test$linreg_residual ** 2))
```



```{r}

#Plot Test Data
ggplot(test, aes(overall_delta, linreg_residual, label = Player)) +
  geom_point()
  #geom_text_repel(aes(label=ifelse(linreg_residual>15,as.character(Player),''))) +
  #geom_abline(slope=1, intercept = 0, color = 'black') +
  #geom_text(aes(50, 90, label = "Actual Score Worse than Predicted Score",), color = 'red') +
  #geom_text(aes(80, 30, label = "Actual Score Better than Predicted Score"), color = 'lightgreen') +
  #labs(title = "Player Year 1 (2nd of 2 Consec. Years) WR Score vs Linear Regression Year 1 WR Score Prediction",
  #       subtitle = "Only Test Data graphed, Players labeled have incorrect predictions of 15+",
  #       x = "Actual Year 1 WR Score", 
  #       y = "Predicted Year 1 WR Score") +
  #theme_minimal() +
  #  xlim(20,100) +
  #  ylim(20, 100)
```


Test data shoes it does generally well predicting scores, especially at the lower end. Typically higher scores are not predicted well, especially if they had a significant increase from the previous year



Given that, lets see if we give the model all of their scores between 2017-2021 to see if it can predict 2022.

```{r}
data_overalls = data[, c("Player", "X2017_Overall", "X2018_Overall", "X2019_Overall", "X2020_Overall", "X2021_Overall", "X2022_Overall")]
data_overalls_19_22 = data_overalls %>%
  filter_at(vars(X2019_Overall, X2020_Overall, X2021_Overall, X2022_Overall),all_vars(!is.na(.)))

sample <- sample(c(TRUE, FALSE), nrow(data_overalls_19_22), replace=TRUE, prob=c(0.8,0.2))
train_overalls <- data_overalls_19_22[sample, ]
test_overalls <- data_overalls_19_22[!sample, ]

wrscore_overalls.lm <- lm(formula = X2022_Overall ~ X2019_Overall + X2020_Overall + X2021_Overall,
                 data = data_overalls_19_22)

print(summary(wrscore_overalls.lm))


#plot residuals
plot(wrscore_overalls.lm$residuals, pch = 16, col = "red")
```

```{r}
#predictions
test_overalls$linreg_X2022_Overall_prediction = predict(wrscore_overalls.lm, test_overalls)
test_overalls$linreg_residual = abs(test_overalls$X2022_Overall - test_overalls$linreg_X2022_Overall_prediction)
test_overalls[,c('Player', 'X2022_Overall','linreg_X2022_Overall_prediction','linreg_residual')]

#Plot Test Data
ggplot(test_overalls, aes(X2022_Overall, linreg_X2022_Overall_prediction, label = Player)) +
  geom_point() + 
  geom_text_repel(aes(label=ifelse(linreg_residual>15,as.character(Player),''))) +
  geom_abline(slope=1, intercept = 0, color = 'black') +
  geom_text(aes(50, 90, label = "Actual Score Worse than Predicted Score",), color = 'red') +
  geom_text(aes(80, 30, label = "Actual Score Better than Predicted Score"), color = 'lightgreen') +
  labs(title = "2022 Predicted Linear Regression WR Score vs Actual 2022 WR Score Prediction",
         subtitle = "Only Test Data graphed, Players labeled have incorrect predictions of 15+",
         x = "Actual 2022 WR Score", 
         y = "Predicted 2022 WR Score") +
  theme_minimal() +
    xlim(20,100) +
    ylim(20, 100)

ggsave("../Images/LinearRegression_2022Predictions.png", bg="white")
```

Consistently, players at the top have their actual score better than their predicted score, suggesting that our linear model might be best when does in groups.

-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

We'll use decision trees to analyze it as a whole to start

Decision Tree
```{r}
library(tidymodels)

tree_spec <- decision_tree() %>%
 set_engine("rpart") %>%
 set_mode("regression")

# Fit the model to the training data
tree_fit <- tree_spec %>%
 fit(formula = Y1_Overall ~ Y0_Yards + Y0_YardsRun + Y0_Targets + Y0_YardsPerRoute + Y0_CatchScore + Y0_OpenScore + Y0_YACScore + Y0_Overall + team_change,
                 data = train)

# Make predictions on the testing data
predictions <- tree_fit %>%
 predict(test) %>%
 pull(.pred)

# Calculate RMSE and R-squared
# metrics <- metric_set(rmse, rsq)
model_performance <- test %>%
 mutate(predictions = predictions) %>%
 metrics(truth = Y1_Overall, estimate = predictions)

print(model_performance)
```
A good amount worse than the linear regression model (this: 0.19, linear regression 0.33), lets see how it visualizes

```{r}
# Load the library
library(rpart.plot)

# Plot the decision tree
rpart.plot(tree_fit$fit, type = 5, extra = 101, under = TRUE, cex = 0.8, box.palette = "auto")
```

Try it without the advanced stats
```{r}
tree_spec <- decision_tree() %>%
 set_engine("rpart") %>%
 set_mode("regression")

# Fit the model to the training data
tree_fit <- tree_spec %>%
 fit(formula = Y1_Overall ~ Y0_Yards + Y0_YardsRun + Y0_Targets + Y0_YardsPerRoute + team_change,
                 data = train)

# Make predictions on the testing data
predictions <- tree_fit %>%
 predict(test) %>%
 pull(.pred)

# Calculate RMSE and R-squared
# metrics <- metric_set(rmse, rsq)
model_performance <- test %>%
 mutate(predictions = predictions) %>%
 metrics(truth = Y1_Overall, estimate = predictions)

print(model_performance)


# Plot the decision tree
rpart.plot(tree_fit$fit, type = 5, extra = 101, under = TRUE, cex = 0.8, box.palette = "auto", roundint=FALSE)
```
We get to 0.22, which is better than the initial model, but still worse than the linear regression.

the model generally tells us that yards gained and yards per route are pretty big indicators for a reciever's overall score.

But with one train/test split, we are overfitting. Lets try using a random forest


```{r}
library(randomForest)
library(caret)

rf <- randomForest(formula = Y1_Overall ~ Y0_Yards + Y0_YardsRun + Y0_Targets + Y0_YardsPerRoute + Y0_CatchScore + Y0_OpenScore + Y0_YACScore + Y0_Overall + team_change,
                 data = train) 

print(rf)
```

```{r}
p2 <- predict(rf, test)
plot(rf)


test$rf_preds = p2
test$rf_error = abs(test$Y1_Overall - test$rf_preds)

#rmse
sqrt(mean(test$rf_error ** 2))
```

A lower RMSE than just the decision tree on its own. Lets see how the predictions do overall


```{r}
ggplot(test, aes(Y1_Overall, rf_preds)) + 
  geom_point() +
  geom_text_repel(aes(label=ifelse(rf_error>15,as.character(Player),''))) +
  geom_abline(slope=1, intercept = 0, color = 'black') +
  geom_text(aes(50, 90, label = "Actual Score Worse than Predicted Score",), color = 'red') +
  geom_text(aes(80, 30, label = "Actual Score Better than Predicted Score"), color = 'lightgreen') +
  labs(title = "2022 Actual WR Score vs Random Forest 2022 WR Score Prediction",
         subtitle = "Only Test Data graphed, Players labeled have incorrect predictions of 15+",
         x = "Actual WR Score", 
         y = "RF Predicted WR Score") +
  theme_minimal() +
    xlim(20,100) +
    ylim(20, 100)

ggsave("../Images/RandomForest_Y1Predictions.png", bg="white")

```

Similar errors and names as we saw before, but now we see a third TE join the greater than 15 error group, showing the volatility of TEs.

```{r}
ggplot(test, aes(Y1_Overall, rf_error)) + 
  geom_point()
```

But the key common trend we see is that the model can predict the middle recievers well (even big drop-offs like Jakobi Meyers 20-21 falling from 88 to 66). But overall, the model regresses most players to the mean, even if super high (Michael Thomas 18-19 went from 96 to 94 but model predicted 74 to 70).


However, the model trees treats output as ints / categories rather than continuous, making this pretty useless, so lets use decision trees that 
a) predict in groups
b) can predict as continuous rather than as integers 


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------


Now we'll use Regression Trees to create regressions for individual groups 

```{r}
library(rsample)     # data splitting 
library(dplyr)       # data wrangling
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
library(caret)       # bagging
```

```{r}
model1 <- rpart(
   formula = Y1_Overall ~ Y0_Yards + Y0_YardsRun + Y0_Targets + Y0_YardsPerRoute + Y0_CatchScore + Y0_OpenScore + Y0_YACScore + Y0_Overall + team_change,
   data = train,
   method = 'anova'
)
rpart.plot(model1)

```

```{r}
plotcp(model1)
```

An ever increasing cp, indicating with more trees come with more losses. Lets build out a full tree to see if it still exists

```{r}
model2 <- rpart(
    formula = Y1_Overall ~ Y0_Yards + Y0_YardsRun + Y0_Targets + Y0_YardsPerRoute + Y0_CatchScore + Y0_OpenScore + Y0_YACScore + Y0_Overall + team_change,
   data = train,
   method = 'anova',
    control = list(cp = 0, xval = 50)
)

plotcp(model2)
abline(v = 5, lty = "dashed")


model2$cptable

```

5 splits seems to be ideal, so we will keep it to that.

Now lets tune minsplit and maxdepth
Full code credit here to https://uc-r.github.io/regression_trees

```{r}
##grid search to find best combination

hyper_grid <- expand.grid(
  minsplit = seq(3, 20, 1),
  maxdepth = seq(3, 15, 1)
)

models <- list()

for (i in 1:nrow(hyper_grid)) {
  
  # get minsplit, maxdepth values at row i
  minsplit <- hyper_grid$minsplit[i]
  maxdepth <- hyper_grid$maxdepth[i]

  # train a model and store in the list
  models[[i]] <- rpart(
    formula = Y1_Overall ~ Y0_Yards + Y0_YardsRun + Y0_Targets + Y0_YardsPerRoute + Y0_CatchScore +
      Y0_OpenScore + Y0_YACScore + Y0_Overall + team_change,
   data = train,
   method = 'anova',
    control = list(minsplit = minsplit, maxdepth = maxdepth)
    )
}

# function to get optimal cp
get_cp <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  cp <- x$cptable[min, "CP"] 
}

# function to get minimum error
get_min_error <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  xerror <- x$cptable[min, "xerror"] 
}

hyper_grid %>%
  mutate(
    cp    = purrr::map_dbl(models, get_cp),
    error = purrr::map_dbl(models, get_min_error)
    ) %>%
  arrange(error) %>%
  top_n(-5, wt = error)
```

```{r}
optimal_tree <- rpart(
    formula = Y1_Overall ~ Y0_Yards + Y0_YardsRun + Y0_Targets + Y0_YardsPerRoute + Y0_CatchScore +
      Y0_OpenScore + Y0_YACScore + Y0_Overall + team_change,
   data = train,
   method = 'anova',
   control = list(minsplit = 5, maxdepth = 6, cp = 0.01688919)
   )

pred <- predict(optimal_tree, newdata = test)
test$regtree_preds = pred
test$regtree_error = abs(test$Y1_Overall - test$regtree_preds)
RMSE(pred = pred, obs = test$Y1_Overall)
```

RMSE still high, pretty close to both the linear regression and simple random forest. Lets see if it has the same problems as them with grouping.

```{r}
test[, c('Player', 'Y0_Overall', 'Y1_Overall', 'linreg_Y1_Overall_prediction', 'rf_preds', 'regtree_preds', 'regtree_error')]


ggplot(test, aes(Y1_Overall, regtree_preds)) + 
  geom_point() +
  geom_text_repel(aes(label=ifelse(regtree_preds>20,as.character(Player),''))) +
  geom_abline(slope=1, intercept = 0, color = 'black') +
  geom_text(aes(50, 90, label = "Actual Score Worse than Predicted Score",), color = 'red') +
  geom_text(aes(80, 30, label = "Actual Score Better than Predicted Score"), color = 'lightgreen') +
  labs(title = "2022 Actual WR Score vs Regression Tree 2022 WR Score Prediction",
         subtitle = "Only Test Data graphed, Players labeled have incorrect predictions of 15+",
         x = "Actual WR Score", 
         y = "RegTree Predicted WR Score") +
  theme_minimal() +
    xlim(20,100) +
    ylim(20, 100)

ggsave("../Images/RegressionTree_Y1Predictions.png", bg="white")

```
Does a better job with some of the worse outliers, but still not able to detect breakouts.


Lets do some bagging to split the dataset and try to remove the high variance we are seeing

Bagging with ipred
```{r}
set.seed(502)

bagged_m1 <- bagging(
  formula = Y1_Overall ~ Y0_Yards + Y0_YardsRun + Y0_Targets + Y0_YardsPerRoute + Y0_CatchScore +
      Y0_OpenScore + Y0_YACScore + Y0_Overall + team_change,
   data = train,
  coob = TRUE
  )

bagged_m1
```

Find number of trees we want to create
```{r}
# assess 10-50 bagged trees
ntree <- 10:100

# create empty vector to store OOB RMSE values
rmse_vals <- vector(mode = "numeric", length = length(ntree))

for (i in seq_along(ntree)) {
  # reproducibility
  set.seed(502)
  
  # perform bagged model
  model <- bagging(
  formula = Y1_Overall ~ Y0_Yards + Y0_YardsRun + Y0_Targets + Y0_YardsPerRoute + Y0_CatchScore +
      Y0_OpenScore + Y0_YACScore + Y0_Overall + team_change,
   data = train,
  coob = TRUE,
  nbagg   = ntree[i]
)
  # get OOB error
  rmse_vals[i] <- model$err
}

plot(ntree, rmse_vals, type = 'l', lwd = 2)
abline(v = 52, col = "red", lty = "dashed")

```

Bagging with caret
```{r}
# Specify 10-fold cross validation
ctrl <- trainControl(method = "cv",  number = 10) 

# CV bagged model
bagged_cv <- train(
 form = Y1_Overall ~ Y0_Yards + Y0_YardsRun + Y0_Targets + Y0_YardsPerRoute + Y0_CatchScore +
      Y0_OpenScore + Y0_YACScore + Y0_Overall + team_change,
   data = train,
  method = "treebag",
  trControl = ctrl,
  importance = TRUE
  )

# assess results
bagged_cv

plot(varImp(bagged_cv), 8)
```

```{r}
pred <- predict(bagged_cv, test)
test$baggedtree_preds = pred
test$baggedtree_error = abs(test$Y1_Overall - test$baggedtree_preds)
RMSE(pred, test$Y1_Overall)
```

Ok but still worse than linear regression


```{r}
ggplot(test, aes(Y1_Overall, baggedtree_preds)) + 
  geom_point() +
  geom_text_repel(aes(label=ifelse(baggedtree_error>15,as.character(Player),''))) +
  geom_abline(slope=1, intercept = 0, color = 'black') +
  geom_text(aes(50, 90, label = "Actual Score Worse than Predicted Score",), color = 'red') +
  geom_text(aes(80, 30, label = "Actual Score Better than Predicted Score"), color = 'lightgreen') +
  labs(title = "2022 Predicted Bagged Tree WR Score vs Actual 2022 WR Score Prediction",
         subtitle = "Only Test Data graphed, Players labeled have incorrect predictions of 15+",
         x = "Actual WR Score", 
         y = "Bagged Tree Predicted WR Score") +
  theme_minimal() +
    xlim(20,100) +
    ylim(20, 100)
```

Got better at the middle predictions, worse at the outer predictions.


-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------



